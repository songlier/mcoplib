// 2025 - Modified by MetaX Integrated Circuits (Shanghai) Co., Ltd. All Rights Reserved.
#include <stddef.h>
#include <torch/all.h>
#include <ATen/cuda/CUDAContext.h>
#include <c10/cuda/CUDAGuard.h>
#ifdef USE_MACA
#include "mctlass/mctlass.h"
#include "mctlass/mctlass_ex.h"
#include "mctlass/epilogue/thread/scale_type.h"
#include "mctlass/frontend_op/mctlass_gemm_scale.h"
#else
#include "cutlass/cutlass.h"
#endif


int64_t cutlass_moe_mm_gemm_kernel_m_w8a8_sm75(int64_t num_valid_tokens, int64_t N, int64_t K, int64_t group) {
    using ElementA = int8_t;
    using ElementB = int8_t;
    using ElementC = maca_bfloat16;
    using ElementCompute = float;
    using LayoutA = mctlass::layout::RowMajor;
    using LayoutB = mctlass::layout::ColumnMajor;
    using LayoutC = mctlass::layout::RowMajor;
    using ArchTag = mctlass::arch::Sm80;

    using mctlassMoeGemmOp = mctlassMoeGemm<
    ElementA,
    LayoutA,
    ElementB,
    LayoutB,
    ElementC,
    LayoutC,
    ElementCompute,
    ArchTag
    >;

    mctlassMoeGemmOp mctlass_op;
    mctlass::gemm::BatchedGemmCoord problem_size(num_valid_tokens, N, K, group);
    mctlass::gemm::GemmCoord kernel_size;
    mctlass::Status status = mctlass_op.gemm_kernel_mnk(problem_size, kernel_size);
    if (status != mctlass::Status::kSuccess) {
        return -1;
    }

    return static_cast<int64_t>(kernel_size.m());
}

void cutlass_moe_mm_w8a8_sm75(torch::Tensor const& a, torch::Tensor const& b, torch::Tensor& c,
                              torch::Tensor const& a_scales, torch::Tensor const& b_scales, torch::Tensor const& moe_weight,
                              torch::Tensor const& token_ids, torch::Tensor const& expert_ids, 
                              torch::Tensor const& num_tokens_post_padded, 
                              int64_t N, int64_t K, int64_t EM, int64_t num_valid_tokens, int64_t topk, bool mul_routed_weight) {
    using ElementA = int8_t;
    using ElementB = int8_t;
    using ElementC = maca_bfloat16;
    using ElementCompute = float;
    using LayoutA = mctlass::layout::RowMajor;
    using LayoutB = mctlass::layout::ColumnMajor;
    using LayoutC = mctlass::layout::RowMajor;
    using ArchTag = mctlass::arch::Sm80;

    using mctlassMoeGemmOp = mctlassMoeGemm<
    ElementA,
    LayoutA,
    ElementB,
    LayoutB,
    ElementC,
    LayoutC,
    ElementCompute,
    ArchTag
    >;

    int64_t num_experts = b.size(0);

    auto a_ptr = static_cast<ElementA const*>(a.data_ptr());
    auto b_ptr = static_cast<ElementB const*>(b.data_ptr());
    auto c_ptr = static_cast<ElementC*>(c.data_ptr());
    auto scale_a_ptr = a_scales.data_ptr<float>();
    auto scale_b_ptr = b_scales.data_ptr<float>();
    auto moe_weight_ptr = moe_weight.data_ptr<float>();
    auto token_ids_ptr = token_ids.data_ptr<int32_t>();
    auto expert_ids_ptr = expert_ids.data_ptr<int32_t>();
    auto num_tokens_post_padded_ptr = num_tokens_post_padded.data_ptr<int32_t>();

    mctlassMoeGemmOp mctlass_op;
    mctlass::gemm::BatchedGemmCoord problem_size(num_valid_tokens, N, K, num_experts);

    const cudaStream_t stream = at::cuda::getCurrentCUDAStream();
    // auto stream = at::cuda::getCurrentCUDAStream(a.get_device());
    // mctlass
    typename mctlassMoeGemmOp::Arguments arguments{
        mctlass::gemm::GemmUniversalMode::kGemm,
        problem_size,
        {scale_a_ptr, scale_b_ptr, moe_weight_ptr},
        a_ptr,
        b_ptr,
        c_ptr,
        {token_ids_ptr,
        expert_ids_ptr,
        num_tokens_post_padded_ptr,
        static_cast<int32_t>(EM),
        static_cast<int32_t>(topk),
        mul_routed_weight}};
    
    mctlass::Status is_success = mctlass_op(arguments, NULL, stream);
    assert(mctlass::Status::kSuccess == is_success);
}

int64_t cutlass_moe_mm_gemm_kernel_m_w8a8(int64_t num_valid_tokens, 
                                          int64_t N, 
                                          int64_t K, 
                                          int64_t group) 
{
  return cutlass_moe_mm_gemm_kernel_m_w8a8_sm75(num_valid_tokens, N, K, group);
}

void cutlass_moe_mm_w8a8(torch::Tensor const& a, 
                         torch::Tensor const& b, 
                         torch::Tensor& c,
                         torch::Tensor const& a_scales, 
                         torch::Tensor const& b_scales, 
                         torch::Tensor const& moe_weight,
                         torch::Tensor const& token_ids, 
                         torch::Tensor const& expert_ids, 
                         torch::Tensor const& num_tokens_post_padded, 
                         int64_t N, 
                         int64_t K, 
                         int64_t EM, 
                         int64_t num_valid_tokens, 
                         int64_t topk, 
                         bool mul_routed_weight) 
{

  cutlass_moe_mm_w8a8_sm75(a, b, c, a_scales, b_scales, moe_weight, token_ids, expert_ids,
                           num_tokens_post_padded, N, K, EM, num_valid_tokens, topk, mul_routed_weight);
}